{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ladlgz8sqlcN"
      },
      "source": [
        "**Establish Connection to Dataset Zip on Google Drive**\n",
        "\n",
        "This will allow us to directly reference the stored landuseDataset.zip within our drive, and unzip it within Google Colab's current session.\n",
        "\n",
        "file_id = 1R0z-1tAf9YNIWuyd49cPPCHaCiNqN319\n",
        "\n",
        "file_name = landUseDataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv0KrYGZqED6"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import io\n",
        "\n",
        "def download_dataset_from_google_drive(file_id = '1R0z-1tAf9YNIWuyd49cPPCHaCiNqN319', file_name = 'landUseDataset.zip'):\n",
        "  # Authenticate and create the Drive API service\n",
        "  auth.authenticate_user()\n",
        "  drive_service = build('drive', 'v3')\n",
        "\n",
        "  request = drive_service.files().get_media(fileId=file_id)\n",
        "  fh = io.BytesIO()\n",
        "  downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "  done = False\n",
        "  while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(f\"Download {int(status.progress() * 100)}%\")\n",
        "\n",
        "  with open(file_name, 'wb') as f:\n",
        "    f.write(fh.getvalue())\n",
        "\n",
        "  import zipfile\n",
        "  import os\n",
        "  extract_path = '/content/'\n",
        "\n",
        "  with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "  print(f\"Contents of {file_name} have been extracted to {extract_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfr8HYcDXYOe"
      },
      "source": [
        "**Steps For Development:**\n",
        "\n",
        "1. Load data from /content/UCMerced_LandUse/*\n",
        "\n",
        "  i. Sort into various assortments of training/testing subsets\n",
        "\n",
        "2. Apply image preprocessing.\n",
        "\n",
        "  i. Apply noise at a variety of levels\n",
        "\n",
        "3. Define the neural network models\n",
        "\n",
        "  i. I.e. Define resnet-50, ViT, and ConvNeXt-Tiny\n",
        "\n",
        "4. Train the models on the pre-sorted training subsets\n",
        "\n",
        "5. Test the models on the pre-sorted testing subsets\n",
        "\n",
        "6. Collect metrics\n",
        "\n",
        "  i. F1\n",
        "\n",
        "  ii. Accurracy\n",
        "\n",
        "  iii. Precision\n",
        "\n",
        "  iv. Resource usage\n",
        "\n",
        "  v. etc.\n",
        "\n",
        "7. Visualize metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTU8dRMOZG4U"
      },
      "source": [
        "# **1. Load and Preprocess Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72TbM1KlZGbx"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "import random\n",
        "\n",
        "# General dataset constants\n",
        "DATASET_PATH = pathlib.Path('/content/UCMerced_LandUse/Images')\n",
        "IMG_DIMENSION = 200\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Helper function to load images\n",
        "def load_data(dataset_path=DATASET_PATH, img_width=IMG_DIMENSION, img_height=IMG_DIMENSION, sampling_rate=1.0):\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  desired_classes = {\n",
        "      \"Nature\": [\"beach\", \"river\", \"forest\", \"chaparral\"],\n",
        "      \"Human Made\": [\"buildings\", \"denseresidential\", \"tenniscourt\", \"mobilehomepark\"]\n",
        "  }\n",
        "\n",
        "  class_dirs = sorted(dataset_path.iterdir())\n",
        "  for class_dir in class_dirs:\n",
        "    class_name = class_dir.name.lower()\n",
        "\n",
        "    for category, class_list in desired_classes.items():\n",
        "      if class_name in class_list:\n",
        "        image_paths = sorted(class_dir.iterdir())\n",
        "\n",
        "        sample_image_paths = random.sample(image_paths, int(len(image_paths) * sampling_rate))\n",
        "        for image_path in sample_image_paths:\n",
        "          try:\n",
        "            img = cv2.imread(str(image_path))\n",
        "            if img is not None:\n",
        "              img = cv2.resize(img, (IMG_DIMENSION, IMG_DIMENSION))\n",
        "              images.append(img)\n",
        "              labels.append(0 if category == \"Nature\" else 1)  # 0 for Nature, 1 for Human Made\n",
        "\n",
        "            else:\n",
        "              print(f\"Error loading image: {image_path.name}\")\n",
        "          except Exception as e:\n",
        "            print(f\"Error loading image: {image_path.name}, Error: {e}\")\n",
        "        break\n",
        "\n",
        "  images = preprocess_input(np.array(images).astype('float32'))\n",
        "  labels = np.array(labels)\n",
        "  return images, labels\n",
        "\n",
        " # Create data subsets for experiments\n",
        "def create_subset(X, y, percentage):\n",
        "  subset_size = int(len(X) * percentage)\n",
        "  return X[:subset_size], y[:subset_size]\n",
        "\n",
        "# Add noise to image\n",
        "def add_noise(image):\n",
        "  noisy_image = image + 0.2 * np.random.normal(loc=0.0, scale=1.0, size=image.shape)\n",
        "  noisy_image = np.clip(noisy_image, 0.0, 1.0)\n",
        "  return noisy_image\n",
        "\n",
        "# Add noise to dataset as a whole\n",
        "def apply_noise_to_dataset(X, noise_percentage):\n",
        "  num_noisy_images = int(len(X) * noise_percentage)\n",
        "  indices_to_noise = random.sample(range(len(X)), num_noisy_images)\n",
        "  X_noisy = X.copy()\n",
        "  for i in indices_to_noise:\n",
        "    X_noisy[i] = add_noise(X_noisy[i])\n",
        "  return X_noisy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjeiMukInIQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29873b0-174a-4e81-9ea5-a315f9ccc902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download 31%\n",
            "Download 63%\n",
            "Download 94%\n",
            "Download 100%\n",
            "Contents of landUseDataset.zip have been extracted to /content/\n"
          ]
        }
      ],
      "source": [
        "if not DATASET_PATH.exists():\n",
        "  download_dataset_from_google_drive()\n",
        "\n",
        "# Retrieve the images and classifications of our dataset\n",
        "images, labels = load_data()\n",
        "\n",
        "# Split the data into training, validation, and test sets.\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQCGQjiy6VOg"
      },
      "source": [
        "# **2. Define Model Architectures**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_rwDWvp4W2K",
        "outputId": "a78b1a82-6c70-4619-a371-3efa30b57dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50, ConvNeXtTiny\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "import time as tm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fabTKh8a6dSF"
      },
      "source": [
        "### **ResNet-50 Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeHZwihn6HP2"
      },
      "outputs": [],
      "source": [
        "def build_resnet50(input_shape=(IMG_DIMENSION, IMG_DIMENSION, 3), num_classes=NUM_CLASSES):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(IMG_DIMENSION, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rCAgy0l6lpR"
      },
      "source": [
        "### **Vision Transformer (ViT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W25Azad6yq8"
      },
      "outputs": [],
      "source": [
        "def build_vit(input_shape=(IMG_DIMENSION, IMG_DIMENSION, 3), num_classes=NUM_CLASSES):\n",
        "    vit_layer = keras.applications.efficientnet_v2.EfficientNetV2S(\n",
        "        weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
        "    )\n",
        "    vit_layer.trainable = True  # Freeze backbone\n",
        "\n",
        "    x = GlobalAveragePooling2D()(vit_layer.output)\n",
        "    x = Dense(IMG_DIMENSION, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=vit_layer.input, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsBMvMxr617V"
      },
      "source": [
        "### **ConvNeXt-Tiny**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mIREDZY661I"
      },
      "outputs": [],
      "source": [
        "def build_convnext_tiny(input_shape=(IMG_DIMENSION, IMG_DIMENSION, 3), num_classes=NUM_CLASSES):\n",
        "    base_model = ConvNeXtTiny(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True  # Freeze layers\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(IMG_DIMENSION, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0EnnyPK7iFC"
      },
      "source": [
        "# **Train Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_KLSvL07vF1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/IMG_DIMENSION,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Extend train_model to evaluate and return performance metrics\n",
        "def train_and_evaluate(model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "                       epochs=5, batch_size=32, experiment_name=\"experiment\"):\n",
        "    train_generator = datagen.flow(X_train, y_train, batch_size=batch_size, subset='training')\n",
        "    val_generator = datagen.flow(X_train, y_train, batch_size=batch_size, subset='validation')\n",
        "\n",
        "    start_time = tm.time()\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1, callbacks=[reduce_lr, early_stop]\n",
        "    )\n",
        "    training_time = tm.time() - start_time\n",
        "\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "    y_true = y_test\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    error_rate = 1 - accuracy\n",
        "\n",
        "    return {\n",
        "        \"Experiment\": experiment_name,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Recall\": recall,\n",
        "        \"F1\": f1,\n",
        "        \"Precision\": precision,\n",
        "        \"Error Rate\": error_rate,\n",
        "        \"Training Time\": training_time\n",
        "    }, history, y_pred, y_true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ9t6Udw6hw-"
      },
      "source": [
        "# **Visualize Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9exZjPD26hLm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def loss_and_accuracy_graph(history, experiment_name):\n",
        "  training_loss = history.history['loss']\n",
        "  test_loss = history.history['val_loss']\n",
        "\n",
        "  # Create count of the number of epochs\n",
        "  epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "  # Visualize loss history\n",
        "  plt.plot(epoch_count, training_loss, 'r--')\n",
        "  plt.plot(epoch_count, test_loss, 'b-')\n",
        "  plt.legend(['Training Loss', 'Test Loss'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title(f\"Epoch vs. Loss {experiment_name}\")\n",
        "  ax = plt.gca()\n",
        "  ax.set_ylim([0, None])\n",
        "  plt.show();\n",
        "\n",
        "  training_accuracy = history.history['accuracy']\n",
        "  test_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  # Create count of the number of epochs\n",
        "  epoch_count = range(1, len(training_accuracy) + 1)\n",
        "\n",
        "  # Visualize loss history\n",
        "  plt.plot(epoch_count, training_accuracy, 'r--')\n",
        "  plt.plot(epoch_count, test_accuracy, 'b-')\n",
        "  plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title(f\"Epoch vs. Accuracy {experiment_name}\")\n",
        "  ax = plt.gca()\n",
        "  ax.set_ylim([0, None])\n",
        "  plt.show();\n",
        "\n",
        "def conf_matrix(y_pred, y_val, experiment_name):\n",
        "  mat = confusion_matrix(y_val, y_pred)\n",
        "  sns.heatmap(mat, xticklabels=['Nature', 'Human Made'],  yticklabels=['Nature', 'Human Made'], square=True, fmt='d', annot=True, cbar=False)\n",
        "  plt.xlabel('Predicted Value')\n",
        "  plt.ylabel('True Value')\n",
        "  plt.title(f\"Confusion Matrix: {experiment_name}\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAexn5Y9JJ_s",
        "outputId": "c8766234-5f2c-434b-de41-98c304f1e4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (4.12.2)\n",
            "\n",
            "------------------------------------------------------\n",
            "             RESOURCE USAGE STATISTICS                \n",
            "------------------------------------------------------\n",
            "\n",
            "Gen RAM Free: 10.9 GB  | Proc size   : 1.8 GB\n",
            "GPU RAM Free: 15092 MB | Used        :     2 MB | Util        :     0% | Total       : 15360MB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Code retrieved from here: https://gist.github.com/darien-schettler/cb0724d9fa453607d48b8b5c2a8e1942\n",
        "\n",
        "def get_colab_usage(pip_install=False, import_libs=True, return_fn=True):\n",
        "    \"\"\" Retrieve Google Colab Resource Utilization Stats\n",
        "\n",
        "    Args:\n",
        "        pip_install (bool, optional): Whether to preform pip installs\n",
        "        import_libs (bool, optional): Whether to import libraries\n",
        "        return_fn (bool, optional): Whether or not to return get_usage fn\n",
        "\n",
        "    Returns:\n",
        "        The get_usage fn ...\n",
        "            (potentially... only if return_fn flag is set to True)\n",
        "            ... which can be used to determine resource utilization stats\n",
        "            at any time in the future of this session without need for\n",
        "            any pip installs or library imports or fn definitions\n",
        "    \"\"\"\n",
        "\n",
        "    if pip_install:\n",
        "        # memory footprint support libraries/code\n",
        "        !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "        !pip install gputil\n",
        "        !pip install psutil\n",
        "        !pip install humanize\n",
        "\n",
        "    if import_libs:\n",
        "        import psutil\n",
        "        import humanize\n",
        "        import os\n",
        "        import GPUtil as GPU\n",
        "\n",
        "    def print_resource_usage():\n",
        "        \"\"\"Function that actually retrieves resource utilization statistics\"\"\"\n",
        "\n",
        "        # Get the activate GPU\n",
        "        # TODO >>> GPU not guaranteed to be the only one <<< TODO\n",
        "        gpu = GPU.getGPUs()[0]\n",
        "\n",
        "        # Get current process\n",
        "        process = psutil.Process(os.getpid())\n",
        "\n",
        "        # Get general ram usage\n",
        "        gen_ram = humanize.naturalsize(psutil.virtual_memory().available)\n",
        "\n",
        "        # Get processor size\n",
        "        proc_size = humanize.naturalsize(process.memory_info().rss)\n",
        "\n",
        "        # Get gpu stats\n",
        "        gpu_free_mem  = gpu.memoryFree\n",
        "        gpu_used_mem  = gpu.memoryUsed\n",
        "        gpu_util_mem  = gpu.memoryUtil*100\n",
        "        gpu_total_mem = gpu.memoryTotal\n",
        "\n",
        "        # Print interpretable resource utilization statistics\n",
        "        print(\"\\n------------------------------------------------------\")\n",
        "        print(\"             RESOURCE USAGE STATISTICS                \")\n",
        "        print(\"------------------------------------------------------\\n\")\n",
        "        print(\"Gen RAM Free: {:8} | \" \\\n",
        "              \"Proc size   : {}\"\\\n",
        "              \"\".format(gen_ram, proc_size))\n",
        "\n",
        "        print(\"GPU RAM Free: {:4.0f} MB | \" \\\n",
        "              \"Used        : {:5.0f} MB | \" \\\n",
        "              \"Util        : {:5.0f}% | \" \\\n",
        "              \"Total       : {:5.0f}MB\\n\" \\\n",
        "              \"\".format(gpu_free_mem, gpu_used_mem,\n",
        "                        gpu_util_mem, gpu_total_mem))\n",
        "\n",
        "    # Internally call the fn\n",
        "    print_resource_usage()\n",
        "\n",
        "    if return_fn:\n",
        "        return(print_resource_usage)\n",
        "\n",
        "# This will print the resource utilization and give us access\n",
        "# to the fn `get_usage` which can now be called like a regular\n",
        "# function with no arguments required.\n",
        "get_usage = get_colab_usage(pip_install=True, return_fn=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG_yJPwY7zfG"
      },
      "source": [
        "# **Execute Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gdO_GqWx7xPT",
        "outputId": "820dc96c-0fe6-4ee1-dd3e-5d1ba58c8da1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'build_resnet50' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-999f500866af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m model_builders = {\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_resnet50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"vit\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_vit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"convnext\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_convnext_tiny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_resnet50' is not defined"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "variables = ['subset', 'epochs', 'noise']\n",
        "\n",
        "\n",
        "default_subset = 1.0\n",
        "default_epochs = 10\n",
        "default_noise = 0.2\n",
        "\n",
        "subset_sizes = [0.3, 0.6, 1.0]\n",
        "epochs_list = [5, 10, 20]\n",
        "noise_percentages = [0.1, 0.2, 0.3]\n",
        "\n",
        "model_builders = {\n",
        "    \"resnet50\": build_resnet50,\n",
        "    \"vit\": build_vit,\n",
        "    \"convnext\": build_convnext_tiny\n",
        "}\n",
        "\n",
        "for sweep_variable in variables:\n",
        "  print(f\" = = = = Running experiment on varying {sweep_variable}. = = = =\")\n",
        "  # Run sweep based on selected variable\n",
        "  if sweep_variable == 'subset':\n",
        "      for subset_percentage in subset_sizes:\n",
        "          X_train_sub, y_train_sub = create_subset(X_train, y_train, subset_percentage)\n",
        "          X_train_sub_noisy = apply_noise_to_dataset(X_train_sub, default_noise)\n",
        "          for model_name, builder in model_builders.items():\n",
        "              print(f\" --- Training {model_name} | {subset_percentage * 100}% subset: {len(X_train_sub_noisy)} images | {default_epochs} epochs | {default_noise * 100}% dataset noise ---\")\n",
        "              model = builder()\n",
        "              metrics, history, y_pred, y_true = train_and_evaluate(\n",
        "                  model, X_train_sub_noisy, y_train_sub, X_val, y_val, X_test, y_test,\n",
        "                  epochs=default_epochs,\n",
        "                  experiment_name=f\"{model_name}_subset{subset_percentage}_epochs{default_epochs}_noise{default_noise}\"\n",
        "              )\n",
        "              results.append(metrics)\n",
        "              loss_and_accuracy_graph(history, metrics[\"Experiment\"])\n",
        "              conf_matrix(y_pred, y_true, metrics[\"Experiment\"])\n",
        "          print(get_colab_usage(pip_install=False, return_fn=True))\n",
        "\n",
        "  elif sweep_variable == 'epochs':\n",
        "      for epoch_count in epochs_list:\n",
        "          X_train_sub, y_train_sub = create_subset(X_train, y_train, default_subset)\n",
        "          X_train_sub_noisy = apply_noise_to_dataset(X_train_sub, default_noise)\n",
        "          for model_name, builder in model_builders.items():\n",
        "              print(f\" --- Training {model_name} | {default_subset}% subset: {len(X_train_sub_noisy)} images | {epoch_count} epochs | {default_noise * 100}% Dataset Noise ---\")\n",
        "              model = builder()\n",
        "              metrics, history, y_pred, y_true = train_and_evaluate(\n",
        "                  model, X_train_sub_noisy, y_train_sub, X_val, y_val, X_test, y_test,\n",
        "                  epochs=epoch_count,\n",
        "                  experiment_name=f\"{model_name}_subset{default_subset}_epochs{epoch_count}_noise{default_noise}\"\n",
        "              )\n",
        "              loss_and_accuracy_graph(history, metrics[\"Experiment\"])\n",
        "              conf_matrix(y_pred, y_true, metrics[\"Experiment\"])\n",
        "              results.append(metrics)\n",
        "          print(get_colab_usage(pip_install=False, return_fn=True))\n",
        "\n",
        "\n",
        "  if sweep_variable == 'noise':\n",
        "      for noise_percentage in noise_percentages:\n",
        "          X_train_sub, y_train_sub = create_subset(X_train, y_train, default_subset)\n",
        "          X_train_sub_noisy = apply_noise_to_dataset(X_train_sub, noise_percentage)\n",
        "          for model_name, builder in model_builders.items():\n",
        "              print(f\" --- Training {model_name} | {default_subset * 100}% subset: {len(X_train_sub_noisy)} images | {default_epochs} epochs | {noise_percentage * 100}% Dataset Noise ---\")\n",
        "              model = builder()\n",
        "              metrics, history, y_pred, y_true = train_and_evaluate(\n",
        "                  model, X_train_sub_noisy, y_train_sub, X_val, y_val, X_test, y_test,\n",
        "                  epochs=default_epochs,\n",
        "                  experiment_name=f\"{model_name}_subset{default_subset}_epochs{default_epochs}_noise{noise_percentage}\"\n",
        "              )\n",
        "              results.append(metrics)\n",
        "              loss_and_accuracy_graph(history, metrics[\"Experiment\"])\n",
        "              conf_matrix(y_pred, y_true, metrics[\"Experiment\"])\n",
        "          print(get_colab_usage(pip_install=False, return_fn=True))\n",
        "\n",
        "  # get_usage = get_colab_usage(pip_install=False, import_libs=False, return_fn=True)\n",
        "  results_df = pd.DataFrame(results)\n",
        "  print(results_df)\n",
        "  results_df.to_csv(f\"{sweep_variable}_results.csv\", index=False)\n",
        "  print(\" = = = = = = \\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}